<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Tag: DeepLearning 吴恩达 | songiapo</title><meta name="author" content="songjiapo"><meta name="copyright" content="songjiapo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="songiapo">
<meta property="og:url" content="https://github.com/songapore/songapore.github.io/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/page/2/index.html">
<meta property="og:site_name" content="songiapo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/songapore/songapore.github.io/img/avatar.png">
<meta property="article:author" content="songjiapo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/songapore/songapore.github.io/img/avatar.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://github.com/songapore/songapore.github.io/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Tag: DeepLearning 吴恩达',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2021-09-05 20:16:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">76</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">30</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">13</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文档</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/posts/21cfbf15/"><span> 🚀 快速开始</span></a></li><li><a class="site-page child" href="/posts/dc584b87/"><span> 📑 主题页面</span></a></li><li><a class="site-page child" href="/posts/4aa8abbe/"><span> 🛠 主题配置-1</span></a></li><li><a class="site-page child" href="/posts/ceeb73f/"><span> ⚔️ 主题配置-2</span></a></li><li><a class="site-page child" href="/posts/98d20436/"><span> ❓ 主题问答</span></a></li><li><a class="site-page child" href="/posts/4073eda/"><span> ⚡️ 进阶教程</span></a></li><li><a class="site-page child" href="/posts/198a4240/"><span> ✨ 更新日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/talking/"><i class="fa-fw fas fa-cubes"></i><span> 自言自语</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/tag.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">songiapo</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文档</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/posts/21cfbf15/"><span> 🚀 快速开始</span></a></li><li><a class="site-page child" href="/posts/dc584b87/"><span> 📑 主题页面</span></a></li><li><a class="site-page child" href="/posts/4aa8abbe/"><span> 🛠 主题配置-1</span></a></li><li><a class="site-page child" href="/posts/ceeb73f/"><span> ⚔️ 主题配置-2</span></a></li><li><a class="site-page child" href="/posts/98d20436/"><span> ❓ 主题问答</span></a></li><li><a class="site-page child" href="/posts/4073eda/"><span> ⚡️ 进阶教程</span></a></li><li><a class="site-page child" href="/posts/198a4240/"><span> ✨ 更新日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/talking/"><i class="fa-fw fas fa-cubes"></i><span> 自言自语</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="page-site-info"><h1 id="site-title">DeepLearning 吴恩达</h1></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left_radius"><a href="/posts/undefined/" title="Mini-batch 梯度下降（Mini-batch gradient descent）-吴恩达 深度学习 course2 2.1~2.2笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mini-batch 梯度下降（Mini-batch gradient descent）-吴恩达 深度学习 course2 2.1~2.2笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="Mini-batch 梯度下降（Mini-batch gradient descent）-吴恩达 深度学习 course2 2.1~2.2笔记">Mini-batch 梯度下降（Mini-batch gradient descent）-吴恩达 深度学习 course2 2.1~2.2笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-27T12:06:10.000Z" title="Created 2018-04-27 20:06:10">2018-04-27</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">Mini-batch 梯度下降（Mini-batch gradient descent）作用机器学习的应用是一个高度依赖经验的过程，伴随着大量迭代的过程，你需要训练诸多模型，才能找到合适的那一个，而优化算法能够帮助你快速训练模型。
深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。
定义
batch 梯度下降法（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新

但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 mini-batch。

mini-batch梯度下降法每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。

工作原理那么究竟mini-batch梯度下降法的原理是什么？在训练集上运行mini-batch梯度下降法，你运行for t=1……5000，因为我们有5000个各有1000个样本的组，在for循环里你要做得基本就是对 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/posts/undefined/" title="梯度检验应用的注意事项（Gradient Checking Implementation Notes）-吴恩达 深度学习 course2 1.14笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="梯度检验应用的注意事项（Gradient Checking Implementation Notes）-吴恩达 深度学习 course2 1.14笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="梯度检验应用的注意事项（Gradient Checking Implementation Notes）-吴恩达 深度学习 course2 1.14笔记">梯度检验应用的注意事项（Gradient Checking Implementation Notes）-吴恩达 深度学习 course2 1.14笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-19T12:40:10.000Z" title="Created 2018-04-19 20:40:10">2018-04-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">在神经网络实施梯度检验的实用技巧和注意事项。

首先，不要在训练中使用梯度检验，它只用于调试。

我的意思是，计算所有i值的dθapprox[i]是一个非常漫长的计算过程，为了实施梯度下降，你必须使用和 backprop来计算，并使用backprop来计算导数，只要调试的时候，你才会计算它，来确认数值是否接近。完成后，你会关闭梯度检验，梯度检验的每一个迭代过程都不执行它，因为它太慢了。

第二点，如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出bug。

也就是说，如果与dθ[i]的值相差很大，我们要做的就是查找不同的i值，看看是哪个导致dθapprox[i]与dθ[i]的值相差这么多。

第三点，在实施梯度检验时，当成本函数包含正则项时，也需要带上正则项进行检验；
第四点，梯度检验不能与dropout同时使用。

因为每次迭代过程中，dropout会随机消除隐藏层单元的不同子集，难以计算dropout在梯度下降上的代价函数。因此dropout可作为优化代价函数的一种方法，但是代价函数J被定义为对所有指数极大的节点子集求和。而在任何迭代过程中，这些节点都有可能被消除，所以很 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/posts/undefined/" title="梯度检验（Gradient checking）-吴恩达 深度学习 course2 1.13笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="梯度检验（Gradient checking）-吴恩达 深度学习 course2 1.13笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="梯度检验（Gradient checking）-吴恩达 深度学习 course2 1.13笔记">梯度检验（Gradient checking）-吴恩达 深度学习 course2 1.13笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-19T12:35:10.000Z" title="Created 2018-04-19 20:35:10">2018-04-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">
梯度检验帮我们节省了很多时间，也多次帮我发现backprop实施过程中的bug，接下来，我们看看如何利用它来调试或检验backprop的实施是否正确。

连接参数
将 W[1]，b[1]，…，W[L]，b[L]全部连接出来，成为一个巨型向量 θ。这样，
1J(W[1],b[1],...,W[L]，b[L])=J(θ)

同时，对 dW[1]，db[1]，…，dW[L]，db[L]执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。
进行梯度检验
现在的问题是dθ 和代价函数J的梯度或坡度有什么关系？


首先，我们要清楚 J 是超参数 θ 的一个函数。
使用双边误差，也就是
$$d\theta_{approx}[i] ＝ \frac{J(\theta_1, \theta_2, …, \theta_i+\varepsilon, …) - J(\theta_1, \theta_2, …, \theta_i-\varepsilon, …)}{2\varepsilon}$$
具体来说，如何定义两个向量是否真的接近彼此？我一般做下列运算，计算这两个向量的距离
$$\frac{||d\th ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/posts/undefined/" title="梯度的数值逼近（Numerical approximation of gradients）-吴恩达 深度学习 course2 1.12笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="梯度的数值逼近（Numerical approximation of gradients）-吴恩达 深度学习 course2 1.12笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="梯度的数值逼近（Numerical approximation of gradients）-吴恩达 深度学习 course2 1.12笔记">梯度的数值逼近（Numerical approximation of gradients）-吴恩达 深度学习 course2 1.12笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-19T12:30:10.000Z" title="Created 2018-04-19 20:30:10">2018-04-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">在实施backprop时，有一个测试叫做梯度检验，它的作用是确保backprop正确实施。因为有时候，你虽然写下了这些方程式，却不能100%确定，执行backprop的所有细节都是正确的。为了逐渐实现梯度检验，我们首先说说如何计算梯度的数值逼近

当 ε 越小时，结果越接近真实的导数，也就是梯度值。
我们可以使用这个方法来检验反向传播是否得以正确实施，如果不正确，它可能有bug需要你来解决。
</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/posts/undefined/" title="神经网络的权重初始化（Weight Initialization for Deep Networks）-吴恩达 深度学习 course2 1.11笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="神经网络的权重初始化（Weight Initialization for Deep Networks）-吴恩达 深度学习 course2 1.11笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="神经网络的权重初始化（Weight Initialization for Deep Networks）-吴恩达 深度学习 course2 1.11笔记">神经网络的权重初始化（Weight Initialization for Deep Networks）-吴恩达 深度学习 course2 1.11笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-19T12:25:10.000Z" title="Created 2018-04-19 20:25:10">2018-04-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">针对梯度消失和梯度爆炸问题，我们想出了一个不完整的解决方案，虽然不能彻底解决问题，却很有用，有助于我们为神经网络更谨慎地选择随机初始化参数。

当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。
要得到较小的 wi，设置Var(wi)=1/n，这里称为 Xavier initialization。
1WL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n)
n就是我喂给的神经单元数量
这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，相应的，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。
将 ReLU 作为激活函数时，Var(wi)=2/n
 总结，当激活函数使用 ReLU 时，Var(wi)=2/n；当激活函数使用 tanh 时，Var(wi)=1/n。
 实际上，我认为所有这些公式只是给你一个起点，它们给出初始化权重矩阵的方差的默认值
</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/posts/undefined/" title="梯度消失 梯度爆炸（Vanishing/ Exploding gradients）-吴恩达 深度学习 course2 1.10笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-10.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="梯度消失 梯度爆炸（Vanishing/ Exploding gradients）-吴恩达 深度学习 course2 1.10笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="梯度消失 梯度爆炸（Vanishing/ Exploding gradients）-吴恩达 深度学习 course2 1.10笔记">梯度消失 梯度爆炸（Vanishing/ Exploding gradients）-吴恩达 深度学习 course2 1.10笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-19T12:20:10.000Z" title="Created 2018-04-19 20:20:10">2018-04-19</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">定义训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。

在深度神经网络中，激活函数以与L相关的指数级数增长或下降的情况,分别称为梯度爆炸或者梯度消失。与层数L相关的导数或梯度函数，也是呈指数级增长或呈指数递减。
发生梯度消失、梯度爆炸的原因为了简单起见，假设我们使用激活函数
$$g(z) = z, b^{[l]} = 0$$
也就是线性激活函数,且忽略了b。对于输出有：
$$\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$$

对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增；
对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减。
虽然我只是讨论了激活函数以与L相关的指数级数增长或下降，它也适用于与层数L相关的导数或梯度函数，也是呈指数级增长或呈指数递减。

最近Microsoft对152层神经网络的研究取得了很大进展，在这样一个深度神经网络中，如果激活函数或梯度函数以与相关的指数增长或递减，它 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/posts/undefined/" title="归一化输入（Normalizing inputs）-吴恩达 深度学习 course2 1.9笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="归一化输入（Normalizing inputs）-吴恩达 深度学习 course2 1.9笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="归一化输入（Normalizing inputs）-吴恩达 深度学习 course2 1.9笔记">归一化输入（Normalizing inputs）-吴恩达 深度学习 course2 1.9笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-18T02:30:10.000Z" title="Created 2018-04-18 10:30:10">2018-04-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">归一化输入的两个步骤训练神经网络，其中一个**加速训练的方法就是归一化输入(Normalizing inputs)**。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：

零均值化

归一化方差；


我们希望无论是训练集和测试集都是通过相同的均值和方差来完成归一化


标准化公式:

$$x = \frac{x - \mu}{\sigma}$$
其中，
$$\mu = \frac{1}{m}\sum^m_{i=1}{x^{(i)}}$$
$$\sigma = \sqrt{\frac{1}{m}\sum^m_{i=1}{x^{(i)^2}}}$$
为什么要归一化特征输入
在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。
</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/posts/undefined/" title="其他正则化方法（Other regularization methods）-吴恩达 深度学习 course2 1.8笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="其他正则化方法（Other regularization methods）-吴恩达 深度学习 course2 1.8笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="其他正则化方法（Other regularization methods）-吴恩达 深度学习 course2 1.8笔记">其他正则化方法（Other regularization methods）-吴恩达 深度学习 course2 1.8笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-17T06:50:00.000Z" title="Created 2018-04-17 14:50:00">2018-04-17</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">除了L2正则化和随机失活（dropout）正则化，还有几种方法可以减少神经网络中的过拟合:
数据扩增（Data Augmentation）
假设你正在拟合猫咪图片分类器，如果你想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。
除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。
通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。
对于字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/posts/undefined/" title="理解 dropout（Understanding Dropout）-吴恩达 深度学习 course2 1.7笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="理解 dropout（Understanding Dropout）-吴恩达 深度学习 course2 1.7笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="理解 dropout（Understanding Dropout）-吴恩达 深度学习 course2 1.7笔记">理解 dropout（Understanding Dropout）-吴恩达 深度学习 course2 1.7笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-17T06:46:00.000Z" title="Created 2018-04-17 14:46:00">2018-04-17</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">直观上理解Dropout可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？


不依赖于任何一个特征，因为该单元的输入可能随时被清除

通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；

实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；

L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。


总结一下，dropout的功能类似于正则化，与正则化不同的是，被应用的方式不同，dropout也会有所不同，甚至更适用于不同的输入范围。
实施dropout的细节
如果你担心某些层比其它层更容易发生过拟合，可以把某些层的keep-prob值设置得比其它层更低， 缺点是为了使用交叉验证，你要搜索更多的超级参数，
另一种方案是在一些层上应用dropout，而有些层不用dropout，应用dropout的层只含有一个超级参数，就是keep-prob。

dropout常在计算机视觉应用中
计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以dropout在计算机视觉中应用得比较频繁，有些计算 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/posts/undefined/" title="dropout 正则化（Dropout Regularization）-吴恩达 深度学习 course2 1.6笔记">     <img class="post_bg" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="dropout 正则化（Dropout Regularization）-吴恩达 深度学习 course2 1.6笔记"></a></div><div class="recent-post-info"><a class="article-title" href="/posts/undefined/" title="dropout 正则化（Dropout Regularization）-吴恩达 深度学习 course2 1.6笔记">dropout 正则化（Dropout Regularization）-吴恩达 深度学习 course2 1.6笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2018-04-17T06:43:00.000Z" title="Created 2018-04-17 14:43:00">2018-04-17</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/DeepLearning/">DeepLearning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">DeepLearning 吴恩达</a></span></div><div class="content">dropout工作原理除了L2 正则化，还有一个非常实用的正则化方法——“Dropout（随机失活）”，我们来看看它的工作原理。
假设你在训练左图这样的神经网络，它存在过拟合，这就是dropout所要处理的，我们复制这个神经网络，dropout会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练。
右图是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。不过可想而知，我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络
如何实施dropout呢如何实施dropout呢？方法有几种，接下来我要讲的是最常用的方法，即inverte ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/">1</a><span class="page-number current">2</span><a class="page-number" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/page/3/">3</a><a class="extend next" rel="next" href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/page/3/"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">songjiapo</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">76</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">30</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">13</div></a></div></div><a class="button--animated" id="card-info-btn" href="https://github.com/songapore/songapore.github.io"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/songapore" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:291946540@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-docs card-categories"><div class="item-headline"><i class="fas fa-car-side"></i><span>文檔目錄</span></div><div class="item-content"><ul class=card-category-list> <li class=card-category-list-item><a class=card-category-list-link href=/posts/21cfbf15/ >🚀 快速開始</a></li> <li class=card-category-list-item><a class=card-category-list-link href=/posts/dc584b87/ >📑 主題頁面</a></li> <li class=card-category-list-item><a class=card-category-list-link href=/posts/4aa8abbe/ >🛠 主題配置-1</a></li> <li class=card-category-list-item><a class=card-category-list-link href=/posts/ceeb73f/ >⚔️ 主題配置-2</a></li> <li class=card-category-list-item><a class=card-category-list-link href=/posts/98d20436/ >❓ 主題問答</a></li> <li class=card-category-list-item><a class=card-category-list-link href=/posts/4073eda/ >⚡️ 進階教程</a></li> <li class=card-category-list-item><a class=card-category-list-link href=/posts/198a4240/ >✨ 更新日誌</a></li> </ul></div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/undefined/" title="udsoncan.connections源码"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="udsoncan.connections源码"/></a><div class="content"><a class="title" href="/posts/undefined/" title="udsoncan.connections源码">udsoncan.connections源码</a><time datetime="2021-09-05T12:12:19.046Z" title="Created 2021-09-05 20:12:19">2021-09-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/undefined/" title="udsoncan-underlying protocol(connections)"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-7.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="udsoncan-underlying protocol(connections)"/></a><div class="content"><a class="title" href="/posts/undefined/" title="udsoncan-underlying protocol(connections)">udsoncan-underlying protocol(connections)</a><time datetime="2021-09-05T12:12:19.046Z" title="Created 2021-09-05 20:12:19">2021-09-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/undefined/" title="Hello World"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/material-2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/posts/undefined/" title="Hello World">Hello World</a><time datetime="2021-09-05T08:33:24.462Z" title="Created 2021-09-05 16:33:24">2021-09-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/7670b080/" title="Butterfly 美化/優化/魔改 教程合集"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/butterfly-diy-cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Butterfly 美化/優化/魔改 教程合集"/></a><div class="content"><a class="title" href="/posts/7670b080/" title="Butterfly 美化/優化/魔改 教程合集">Butterfly 美化/優化/魔改 教程合集</a><time datetime="2021-01-02T10:11:22.000Z" title="Created 2021-01-02 18:11:22">2021-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/ea33ab97/" title="自定義側邊欄"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/aside-diy-cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自定義側邊欄"/></a><div class="content"><a class="title" href="/posts/ea33ab97/" title="自定義側邊欄">自定義側邊欄</a><time datetime="2020-12-30T13:48:10.000Z" title="Created 2020-12-30 21:48:10">2020-12-30</time></div></div></div></div><div class="card-widget" id="card-newest-comments"><div class="item-headline"><i class="fas fa-comment-dots"></i><span>Newest Comments</span></div><div class="aside-list"><span>loading...</span></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DeepLearning/"><span class="card-category-list-name">DeepLearning</span><span class="card-category-list-count">25</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Demo/"><span class="card-category-list-name">Demo</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Docs%E6%96%87%E6%AA%94/"><span class="card-category-list-name">Docs文檔</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Markdown/"><span class="card-category-list-name">Markdown</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Thx/"><span class="card-category-list-name">Thx</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/algorithm/"><span class="card-category-list-name">algorithm</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/git/"><span class="card-category-list-name">git</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/python/"><span class="card-category-list-name">python</span><span class="card-category-list-count">3</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/Aplayer/" style="font-size: 1.1em; color: #999">Aplayer</a> <a href="/tags/Bar/" style="font-size: 1.1em; color: #999">Bar</a> <a href="/tags/Butterfly/" style="font-size: 1.1em; color: #999">Butterfly</a> <a href="/tags/DeepLearning-%E5%90%B4%E6%81%A9%E8%BE%BE/" style="font-size: 1.5em; color: #99a9bf">DeepLearning 吴恩达</a> <a href="/tags/Hexo/" style="font-size: 1.41em; color: #99a5b7">Hexo</a> <a href="/tags/butterfly/" style="font-size: 1.46em; color: #99a7bb">butterfly</a> <a href="/tags/demo/" style="font-size: 1.1em; color: #999">demo</a> <a href="/tags/git-github/" style="font-size: 1.19em; color: #999da1">git github</a> <a href="/tags/highlight/" style="font-size: 1.14em; color: #999b9d">highlight</a> <a href="/tags/live-templates/" style="font-size: 1.1em; color: #999">live templates</a> <a href="/tags/pip/" style="font-size: 1.1em; color: #999">pip</a> <a href="/tags/pipenv/" style="font-size: 1.14em; color: #999b9d">pipenv</a> <a href="/tags/pycharm/" style="font-size: 1.14em; color: #999b9d">pycharm</a> <a href="/tags/pyqt5/" style="font-size: 1.1em; color: #999">pyqt5</a> <a href="/tags/python/" style="font-size: 1.37em; color: #99a4b2">python</a> <a href="/tags/ros%E6%95%99%E7%A8%8B/" style="font-size: 1.32em; color: #99a2ae">ros教程</a> <a href="/tags/ros%E6%95%99%E7%A8%8B-%E7%94%A8ROS%E5%AD%A6%E4%B9%A0%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%BC%96%E7%A8%8B%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%96%B9%E6%B3%95/" style="font-size: 1.23em; color: #999ea6">ros教程 用ROS学习机器人编程的系统方法</a> <a href="/tags/top-img/" style="font-size: 1.1em; color: #999">top_img</a> <a href="/tags/uds/" style="font-size: 1.28em; color: #99a0aa">uds</a> <a href="/tags/udsoncan/" style="font-size: 1.28em; color: #99a0aa">udsoncan</a> <a href="/tags/%E4%B8%BB%E9%A1%8C/" style="font-size: 1.41em; color: #99a5b7">主題</a> <a href="/tags/%E5%84%AA%E5%8C%96/" style="font-size: 1.1em; color: #999">優化</a> <a href="/tags/%E6%89%93%E8%B3%9E/" style="font-size: 1.1em; color: #999">打賞</a> <a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 1.41em; color: #99a5b7">教程</a> <a href="/tags/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/" style="font-size: 1.32em; color: #99a2ae">无人驾驶</a> <a href="/tags/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6-Apollo/" style="font-size: 1.23em; color: #999ea6">无人驾驶 Apollo</a> <a href="/tags/%E6%A8%99%E7%B1%A4%E5%A4%96%E6%8E%9B/" style="font-size: 1.14em; color: #999b9d">標籤外掛</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">算法</a> <a href="/tags/%E7%BE%8E%E5%8C%96/" style="font-size: 1.1em; color: #999">美化</a> <a href="/tags/%E9%AD%94%E6%94%B9/" style="font-size: 1.1em; color: #999">魔改</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/09/"><span class="card-archive-list-date">September 2021</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/01/"><span class="card-archive-list-date">January 2021</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/12/"><span class="card-archive-list-date">December 2020</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/10/"><span class="card-archive-list-date">October 2020</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/07/"><span class="card-archive-list-date">July 2020</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/06/"><span class="card-archive-list-date">June 2020</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/05/"><span class="card-archive-list-date">May 2020</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/03/"><span class="card-archive-list-date">March 2020</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">76</div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2021-09-05T12:16:39.235Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>